{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '2058' (I am process '6127')\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport keras\\nfrom keras import backend as K\\nfrom keras.utils.data_utils import get_file\\nfrom keras.models import Sequential, Model\\nfrom keras.layers.core import Flatten, Dense, Dropout, Lambda\\nfrom keras.layers import Input\\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\\nfrom keras.optimizers import SGD, RMSprop\\nfrom keras.preprocessing import image\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "#import os, json\n",
    "#from glob import glob\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "#from numpy.random import random, permutation\n",
    "#from scipy import misc, ndimage\n",
    "#from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "'''\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/courses/deeplearning1/nbs/'\n",
    "PROJ_HOME_DIR = path + 'data/statefarm/'\n",
    "DATA_HOME_DIR = PROJ_HOME_DIR + 'sample/'\n",
    "\n",
    "results_path = DATA_HOME_DIR + 'results/'\n",
    "test_path = DATA_HOME_DIR + 'test/'\n",
    "valid_path = DATA_HOME_DIR + 'valid/'\n",
    "train_path = DATA_HOME_DIR + 'train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up kaggle config for this competition (after agreeing to rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#kg config -g -u `username` -p `password` \n",
    "#dogs-vs-cats-redux-kernels-edition\n",
    "#state-farm-distracted-driver-detection\n",
    "#\n",
    "#!kg config -c 'state-farm-distracted-driver-detection'\n",
    "#!kg download\n",
    "#!unzip -q sample_submission.csv.zip\n",
    "#!unzip -q imgs.zip\n",
    "#!unzip -q driver_imgs_list.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create directories\n",
    "#%mkdir valid\n",
    "#%mkdir results\n",
    "#%mkdir -p sample/train\n",
    "#%mkdir -p sample/test\n",
    "#%mkdir -p sample/valid\n",
    "#%mkdir -p sample/results\n",
    "#%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and validation set such that a percentage of drivers in the validation set are not in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read names of training images from .csv file\n",
    "#%cd $DATA_HOME_DIR/\n",
    "\n",
    "#driver_imgs_data=pd.read_csv('driver_imgs_list.csv')\n",
    "##train_imgs = driver_imgs_data['img'].tolist()\n",
    "\n",
    "#by_drivers = driver_imgs_data.groupby('subject')\n",
    "#unique_drivers = np.random.permutation(by_drivers.groups.keys())\n",
    "## Set validation set percentage with regards to training set\n",
    "#val_pct = 0.2\n",
    "\n",
    "##random.shuffle(unique_drivers)\n",
    "\n",
    "## These are the drivers we will be entirely moving to the validation set\n",
    "#val_drivers = unique_drivers[:int(len(unique_drivers) * val_pct)]\n",
    "\n",
    "#val_data = driver_imgs_data.loc[driver_imgs_data['subject'].isin(val_drivers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create sub-directories in valid/\n",
    "#for index, row in val_data.iterrows():\n",
    "#    directory = DATA_HOME_DIR+'valid/'+row['classname']\n",
    "#    if not os.path.exists(directory):\n",
    "#        os.mkdir(directory)\n",
    "        \n",
    "## Move validation data from training data\n",
    "#for index, row in val_data.iterrows():\n",
    "#    os.rename(DATA_HOME_DIR+'train/'+row['classname']+'/'+row['img'],\\\n",
    "#              DATA_HOME_DIR+'valid/'+row['classname']+'/'+row['img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from shutil import copyfile\n",
    "\n",
    "#%cd $DATA_HOME_DIR/train\n",
    "#%mkdir ../sample\n",
    "#%mkdir ../sample/train\n",
    "#%mkdir ../sample/valid\n",
    "#for d in glob('c?'):\n",
    "#    os.mkdir('../sample/train/'+d)\n",
    "#    os.mkdir('../sample/valid/'+d)\n",
    "#g = glob('c?/*.jpg')\n",
    "#shuf = np.random.permutation(g)\n",
    "#for i in range(1500): copyfile(shuf[i], '../sample/train/' + shuf[i])\n",
    "\n",
    "#%cd ../valid\n",
    "#g = glob('c?/*.jpg')\n",
    "#shuf = np.random.permutation(g)\n",
    "#for i in range(1000): copyfile(shuf[i], '../sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = get_batches(train_path, batch_size=batch_size)\n",
    "test_batches = get_batches(test_path, batch_size=batch_size*2)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames,\n",
    "    test_filename) = get_classes(DATA_HOME_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer\n",
    "2 conv layers with max pooling followed by a simple dense network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 42s - loss: 1.5490 - acc: 0.5640 - val_loss: 2.3270 - val_acc: 0.1860\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.2905 - acc: 0.9687 - val_loss: 2.1561 - val_acc: 0.1540\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 32s - loss: 0.0876 - acc: 0.9960 - val_loss: 2.1920 - val_acc: 0.1420\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.0413 - acc: 0.9993 - val_loss: 2.2135 - val_acc: 0.2430\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.0235 - acc: 1.0000 - val_loss: 2.2049 - val_acc: 0.2720\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.0150 - acc: 1.0000 - val_loss: 2.1994 - val_acc: 0.3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fe20c6252d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Width shift: move the image left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Height shift: move the image up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random shear angles (max in radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotation: max in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel shift: randomly changing the R,G,B colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.5578 - acc: 0.2053 - val_loss: 3.9461 - val_acc: 0.1010\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 29s - loss: 1.8323 - acc: 0.3880 - val_loss: 2.1643 - val_acc: 0.2000\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 36s - loss: 1.4901 - acc: 0.5120 - val_loss: 2.0587 - val_acc: 0.3060\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 30s - loss: 1.3900 - acc: 0.5353 - val_loss: 2.1295 - val_acc: 0.3260\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 31s - loss: 1.2481 - acc: 0.5960 - val_loss: 2.2162 - val_acc: 0.3550\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 1.1256 - acc: 0.6340 - val_loss: 2.1721 - val_acc: 0.3390\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All image augmentations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.5578 - acc: 0.2053 - val_loss: 3.9461 - val_acc: 0.1010\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 29s - loss: 1.8323 - acc: 0.3880 - val_loss: 2.1643 - val_acc: 0.2000\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 36s - loss: 1.4901 - acc: 0.5120 - val_loss: 2.0587 - val_acc: 0.3060\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 30s - loss: 1.3900 - acc: 0.5353 - val_loss: 2.1295 - val_acc: 0.3260\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 31s - loss: 1.2481 - acc: 0.5960 - val_loss: 2.2162 - val_acc: 0.3550\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 1.1256 - acc: 0.6340 - val_loss: 2.1721 - val_acc: 0.3390\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anneal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 33s - loss: 1.0353 - acc: 0.6553 - val_loss: 2.2600 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 27s - loss: 1.0145 - acc: 0.6687 - val_loss: 2.2322 - val_acc: 0.3060\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 31s - loss: 0.9264 - acc: 0.7153 - val_loss: 2.0182 - val_acc: 0.3710\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 30s - loss: 0.8312 - acc: 0.7367 - val_loss: 2.0489 - val_acc: 0.3830\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.8331 - acc: 0.7447 - val_loss: 2.0979 - val_acc: 0.3810\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 34s - loss: 0.7298 - acc: 0.7667 - val_loss: 1.8315 - val_acc: 0.4740\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.7515 - acc: 0.7707 - val_loss: 1.7906 - val_acc: 0.4880\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 32s - loss: 0.6838 - acc: 0.7907 - val_loss: 1.7497 - val_acc: 0.4980\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 34s - loss: 0.6508 - acc: 0.8007 - val_loss: 1.7098 - val_acc: 0.4990\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 30s - loss: 0.6105 - acc: 0.8200 - val_loss: 1.9481 - val_acc: 0.4420\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.5755 - acc: 0.8260 - val_loss: 1.3890 - val_acc: 0.6210\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.5613 - acc: 0.8380 - val_loss: 1.4182 - val_acc: 0.5830\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 34s - loss: 0.5581 - acc: 0.8233 - val_loss: 1.2925 - val_acc: 0.6130\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.5167 - acc: 0.8453 - val_loss: 1.2745 - val_acc: 0.6070\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.4759 - acc: 0.8620 - val_loss: 1.3986 - val_acc: 0.6140\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 29s - loss: 0.4517 - acc: 0.8653 - val_loss: 1.2350 - val_acc: 0.6150\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 30s - loss: 0.4837 - acc: 0.8507 - val_loss: 1.2108 - val_acc: 0.6510\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.4365 - acc: 0.8793 - val_loss: 1.1052 - val_acc: 0.6860\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 29s - loss: 0.4484 - acc: 0.8693 - val_loss: 1.3291 - val_acc: 0.6000\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.4095 - acc: 0.8827 - val_loss: 1.4257 - val_acc: 0.5740\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.3957 - acc: 0.8787 - val_loss: 1.1816 - val_acc: 0.6620\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.3649 - acc: 0.9000 - val_loss: 1.1900 - val_acc: 0.6420\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.3668 - acc: 0.8933 - val_loss: 1.1068 - val_acc: 0.6740\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.3494 - acc: 0.9073 - val_loss: 1.2784 - val_acc: 0.5500\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.3592 - acc: 0.9027 - val_loss: 1.1305 - val_acc: 0.6540\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 31s - loss: 0.3031 - acc: 0.9233 - val_loss: 0.9704 - val_acc: 0.6870\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 32s - loss: 0.3061 - acc: 0.9153 - val_loss: 1.2099 - val_acc: 0.6470\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 29s - loss: 0.3352 - acc: 0.9067 - val_loss: 1.1271 - val_acc: 0.6350\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 30s - loss: 0.3052 - acc: 0.9153 - val_loss: 1.2400 - val_acc: 0.6490\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.3205 - acc: 0.9067 - val_loss: 1.2416 - val_acc: 0.6160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1dcfcbed0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=30, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(PROJ_HOME_DIR+'sf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(PROJ_HOME_DIR+'sf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-53:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 425, in data_generator_task\n",
      "    generator_output = next(generator)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.py\", line 593, in next\n",
      "    index_array, current_index, current_batch_size = next(self.index_generator)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.py\", line 441, in _flow_index\n",
      "    current_index = (self.batch_index * batch_size) % N\n",
      "ZeroDivisionError: integer division or modulo by zero\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_preds = model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create .csv file for submission to Kaggle\n",
    "You must submit a csv file with the image file name, and a probability for each class.\n",
    "The order of the rows does not matter. The file must have a header and should look like the following:\n",
    "\n",
    "    img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\n",
    "    img_0.jpg,1,0,0,0,0,...,0\n",
    "    img_1.jpg,0.3,0.1,0.6,0,...,0\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'label'), (1, 0.94999999), (2, 0.94999999), (3, 0.94999999), (4, 0.94999999), (5, 0.050000001), (6, 0.050000001), (7, 0.050000001), (8, 0.050000001), (9, 0.050000001), (10, 0.050000001), (11, 0.050000001), (12, 0.94999999), (13, 0.050000001), (14, 0.050000001)]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [int(x[len('unknown/'):-len('.jpg')]) for x in batches.filenames]\n",
    "    \n",
    "list_out = sorted(zip(input_ids,np.clip(preds[:,1],0.05,0.95)))\n",
    "\n",
    "list_out = [('id','label')]+list_out # add column labels\n",
    "\n",
    "print(list_out[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"data/dogscats/kaggle/dogscats_predictions4.csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): www.kaggle.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! kg submit data/dogscats/kaggle/dogscats_predictions4.csv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
